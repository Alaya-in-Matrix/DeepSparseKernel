\section{Introduction}

The Gaussian process (GP) regression is popular as the model provides both predictions and well-calibrated uncertainties. The uncertainty estimation makes the model more robust to unseen events as the model \emph{knows what it knows}. The conventional GP models are usually designed to learn a scalar-valued function. However, in some scenarios, we need to model a vector-valued function, and the multiple outputs are possibly correlated. Instead of treating the vector-valued function as multiple separate scalar-valued functions, the multi-output learning \cite{zhang2017survey} tries to build a unified model and simultaneously learn all the outputs. The overall performance could be enhanced by exploiting the correlations of the tasks.

% Existing methods: convolution based, krok based, GPRN.
Multi-task Gaussian process \cite{vectorvaluedkernel} tries to combine multi-task learning and Gaussian process. The linear model of coregionalization (LMC) methods \cite{journel1978mining} assume the $Q$ outputs $f_i(\bm{x}), i \in \{1\dots Q\}$ are linear combinations of several latent functions as $f_i(\bm{x}) = \sum_{j=1}^U a_{ij} u_j(\bm{x})$. In \cite{bonilla2008multi}, the covariance matrix is expressed as the Kronecker product of the task covariance and the task-irrelevant input covariance matrix. In \cite{nguyen2014collaborative}, the output function $f_i(\bm{x})$ is expressed as the linear combinations of $U$ latent functions $u_j(\bm{x}), j \in \{1\dots U\}$ plus a task-specific function $h_i(\bm{x})$, efficient training and inference methods are also developed. In the Gaussian process regression network (GPRN) model~\cite{wilson2012gaussian}, the $i$-th output function $f_i(\bm{x})$ is also expressed as weighted combinations of $U$ latent functions, however, the weights are also nonlinear functions characterized by GP so that $f_i(\bm{x}) = \sum_{j=1}^U w_{ij}(\bm{x}) (u_j(\bm{x}) + \epsilon_u) + \epsilon_f$ where $\epsilon_u$ and $\epsilon_f$ are the noise terms. Another methodology of building multi-task kernels is \emph{process convolution} \cite{boyle2005dependent,alvarez2009sparse,alvarez2011computationally}, where the output function $f_i(\bm{x})$ is assumed to be the convolution of output-dependent smoothing kernel $g_i(\bm{x})$ and a latent function $u(\bm{x})$.

In this paper, we propose a multi-output Gaussian process model based on the multi-task neural network. As a degenerate GP can be built from a finite feature map and neural networks can generate representative features, a degenerate GP can be derived from a neural network with finite hidden units \cite{lazaro2010marginalized, huang2015scalable}. In our proposed model, the multi-output GP is built from a neural network with shared layers and task-specific layers. The input data is first transformed into \emph{shared features} through the shared layers and further mapped to \emph{task-specific features} by each task's task-specific layers. GP kernels for the outputs are then built from the inner product of the task-specific features. The weights of the multi-task neural network is obtained by maximizing the likelihood with gradient back-propagation.

We compared our model with independent GP model and several multi-output GP models. Three datasets were used, including two public datasets and one dataset from the simulation results of a real-world analog integrated circuit. We demonstrate that our model can provide better predictions and uncertainty estimations than the compared models.

The rest of the paper is organized as follows. In \Fref{sec:Background}, we present the background of the Gaussian process and GP model built from neural networks with finite hidden units. In \Fref{sec:mogp}, we present our proposed multi-output GP model via multi-task neural network. The experimental results are given in \Fref{sec:experiments}. We conclude the paper in \Fref{sec:conclusion}.
% XXX:
% advantage:
%   the correlation modeling is more flexibility
%   efficient

% XXX:
% Kronecker product: the covaiance matrix is decomposed as the Kronecker product of covaiance of task and covaiance of input
% GPRN: $\bm{f}(x) = W(\bm{x})^T \bm{lf}(\bm{x})$, where $\bm{x}$ and $W(\bm{x})$ are GP
% convolution:
