\section{Introduction}

% Gaussian process regression is important, as it models the uncertainty, so the it \emph{know what it knows}.
The Gaussian process (GP) model is popular as the model gives well-calibrated uncertainty. The uncertainty estimation makes the model more robust to unseen events as the model \emph{knows what it knows}. The conventional GP models are usually designed to learn a scalar-valued fuction, however, in some scenarios, we need to model a vector-valued function, and the multiple outputs are possibly correlated. Instead of treating the vector-valued fuction as multiple seperate scalar-valued functions, the multi-output learning\cite{zhang2017survey} tries to build a unified model and simultaneously learn all the outputs, the overall performance could be enhanced by exploitying the correlations of the tasks.

% Existing methods: convolution based, krok based, GPRN.
Multi-task Gaussian process\cite{vectorvaluedkernel} tries to combine multi-task learning and Gaussian process, the linear model of coreginalization (LMC) methods\cite{journel1978mining} assumes the $Q$ outputs $f_i(\bm{x}), i \in \{1\dots Q\}$ are linear combinations of several latent function as $f_i(\bm{x}) = \sum_{j=1}^U a_{ij} u_j(\bm{x})$. In \cite{bonilla2008multi}, the covaricance matrix is expressed as the Kronecker product of the task covaricance and the task-irrelevant input covaiance matrix. In \cite{nguyen2014collaborative}, the output fuction $f_i(\bm{x})$ is expressed as linear combinations of $U$ latent functions $u_j(\bm{x}), j \in \{1\dots U\}$ plus an task-specific function $h_i(\bm{x})$, efficient training and inference methods are also developped. In the Gaussian process regression network (GPRN) model, the $i$-th output fuction $f_i(\bm{x})$ is also expressed as weighted combinations of $U$ latent functions, however, the weights are also nonlinear functions characterized by GP so that $f_i(\bm{x}) = \sum_{j=1}^U w_{ij}(\bm{x}) (u_j(\bm{x}) + \epsilon_u) + \epsilon_f$ where $\epsilon_u$ and $\epsilon_f$ are the noise terms. Another methodology of building multi-task kernels is \emph{process convolution}\cite{boyle2005dependent,alvarez2009sparse,alvarez2011computationally}, where the output function $f_i(\bm{x})$ is assumed to be the convolution of output-dependent smoothing kernel $g_i(\bm{x})$ and a latent function $u(\bm{x})$.  

In this paper, we propose a multi-output Gaussian process model based on multi-task neural network. As a degenerate GP can be build from a finite feature map and neural networks can generate representative features, a degenerate GP can be derived from a neural with finite hidden units\cite{lazaro2010marginalized, huang2015scalable}. In our proposed model, the multi-output GP is built from a deep neural network with shared layers and task-specific layers, the input data is firstly transformed into \emph{shared features} through the shared layers, and further mapped to \emph{task-specific features} by each task's task-specific layers, GP kernels for the outputs are then built from the task-specific features.

We compared our model with independent GP model and several multi-output GP models using three dataset, including two public datasets and one dataset from the simulation results of a real-world analog integrated circuit. We demonstrate that our model can give better predictions than the compared modes. 

The rest of the paper is organized as follows. In \Fref{sec:Background}, we present the background of Gaussian process and GP model built from neural networks with finite hidden units. In \Fref{sec:mogp}, we present our proposed multi-output GP model. The experimental results are given in \Fref{sec:experiments}. We conclude the paper in \Fref{sec:conclusion}. 
% XXX:
% advantage:
%   the correlation modeling is more flexibility
%   efficient

% XXX:
% Kronecker product: the covaiance matrix is decomposed as the Kronecker product of covaiance of task and covaiance of input
% GPRN: $\bm{f}(x) = W(\bm{x})^T \bm{lf}(\bm{x})$, where $\bm{x}$ and $W(\bm{x})$ are GP
% convolution: 
