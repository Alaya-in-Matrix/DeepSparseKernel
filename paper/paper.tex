\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography


% Added by Wenlong Lyu
\usepackage{amsmath}
\usepackage[plain]{fancyref}
\usepackage{bm}
\usepackage{todonotes}
\usepackage{placeins}
\usepackage{multirow}

\title{Efficient Multi-output Gaussian Process Regression using Deep Neural Network with shared layers}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Wenlong Lyu \\
  School of Microelectronics\\
  Fudan University\\
  Pittsburgh, PA 15213 \\
  \texttt{wllv16@fudan.edu.cn} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract} 
    The last hidden layer of a neural network can be viewed as a feature map,
    and a kernel can be constructed from a feature map, so we can construct a
    kernel for Gaussian process regression from a neural network with finite
    hidden units; on the other hands, multiple correlated outputs can be
    represented by a neural network with shared hidden layers. In this paper,
    we build opon these two ideas, and propose a simple multi-output Gaussian
    process regression model, the kernels of multiple tasks are constructed
    from a neural network with shared hidden layers and task-specific layers.
    We compare our model with several state-of-the-art multi-output Gaussian
    process models using two public datasets and two examples of real-world
    analog integrated circuits, the results show that our model is competitive
    compared with these models.
\end{abstract}

\section{Introduction}

\section{Background}

\subsection{GP}

\subsection{NN-GP}

\paragraph{Deep ensemble} wtf

\section{Multi-output Gaussian Process model via Neural network}

\section{Related works}

% XXX:
% DNGO
% Deep-Kernel-Learning
% DNN-GP

\section{Experiment}

\bibliographystyle{unsrt}
\bibliography{ref.bib}

\end{document}
