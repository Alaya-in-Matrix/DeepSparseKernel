\section{Experimental Results}\label{sec:report}

% TODO: acronym of algorithms

We implemented the multi-output Gaussian process model using Python3.5.2, the gradient of the loss function in \fref{eq:mo_likelihood} is calculated by \texttt{autograd} package\cite{maclaurin2015autograd}. We compared our algorithm with several state-of-the-art multi-output GP models on three dataset. The datasets include two public available datasets and one dataset sampled from a real-world analog integrated circuit, all the dataset and the test code are provided in the supplementary materials, and will be made public upon publication.

We use standardized mean squared error (SMSE) and negative log likelihood (NLL) as the evaluation criteria. For all the test cases and compared algorithms, ten independent runs were performed to average random fluctuations, we report both the mean and standard deviation of the SMSE and NLL.

\subsection{The Energy Building (ENB) Dataset}\label{sec:enb}

The ENB dataset used is a small dataset with 768 samples, each sample has eight inputs and two outputs, we use 700 samples as the training data, and the remaining 668 samples are used for testing. The dataset comes from simulations to 768 buildings\cite{spyromitros2016multi, tsanas2012accurate}, the eight inputs are the building parameters like surface area and orientation, while the two outputs are the heating load and cooling load\footnote{The dataset is available at http://mulan.sourceforge.net/datasets-mtr.html}.

For our MTNN-GP model, we use a neural network with two shared layers, and one task-specific layer per output, the tanh function is used as the activation function. For this architecture, we need to learn more than thirty thousands of parameters using only 700 samples. 

The MTNN-GP model is compared with independen GP modeling (IGP) using the \texttt{GPML} package\cite{rasmussen2010gaussian} and four multi-output Gaussian process models, including the collaborative multi-output Gaussian processes (COGP)\footnote{downloaded from https://github.com/trungngv/cogp} proposed in \cite{nguyen2014collaborative}, the sparse convolved Gaussian processes (SCGP)\footnote{downloaded from https://github.com/SheffieldML/multigp} method proposed in \cite{alvarez2009sparse}, and the Gaussian process regression network with NPV and MF inference methods\footnote{downloaded from https://github.com/trungngv/gprn}(GPRN-NPV and GPRN-MF) \cite{nguyen2013efficient}. For the independent GP (IGP), the ARD squared-exponential kernel function is used, for other methods, we used the default algorithm configurations of the corrsponding open source packages.

The SMSE and NLL statistics are given in \fref{tab:result_enb}, the code for the GPRN-NPV and GPRN-MF modes give no predictive variances, so only the SMSE statistics are reported. We can see that our MTNN-GP is better than training independent GP modes for each task, as for other multi-output GP model, they all gave results worse than the IGP model.


\begin{table}[!htb]
    \centering
    \caption{The SMSE and NLL statistics of the ENB dataset}
    \label{tab:result_enb}
    \begin{tabular}{lllll}
        \toprule
        Algo      & Output1(SMSE)          & Output2(SMSE)          & Output1(NLL)        & Output2(NLL)         \\ \midrule
        MTNN-GP:  & 0.00155 $\pm$ 0.000159 & 0.00753 $\pm$ 0.00135  & 0.332 $\pm$ 0.0634  & 0.972 $\pm$ 0.107    \\
        IGP:      & 0.00188 $\pm$ 0        & 0.00911 $\pm$ 0        & 0.538 $\pm$ 0       & 1.01  $\pm$ 0        \\
        GOGP:     & 0.00597 $\pm$ 0.00088  & 0.0144  $\pm$ 0.000831 & 1.34  $\pm$ 0.159   & 2.08  $\pm$ 0.212    \\
        SCGP:     & 0.708   $\pm$ 3.78e-05 & 1.21    $\pm$ 4.1e-05  & 1.56  $\pm$ 0.00363 & 1.66  $\pm$ 0.00063  \\
        GPRN-NPV: & 6.87    $\pm$ 9.36e-16 & 8.63    $\pm$ 1.87e-15 & NA                  & NA                   \\
        GPRN-MF:  & 0.359   $\pm$ 0.225    & 0.614   $\pm$ 0.339    & NA                  & NA                   \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{SARCOS}\label{sec:sarcos}

\begin{itemize}
\item
  44484 training
\item
  4449 test
\item
  21 inputs
\item
  2 outputs
\item
  SCGP: subset of data: use 2000 training set
\item
  GPRN-AVI: NIPS2017, black-box likelihood, M/N = 0.04
\item
  SCGP gives zero prediction, underfitting
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\caption{SMSE of the SARCOS dataset (small is better)}\tabularnewline
\toprule
Algo & Output1 & Output 2\tabularnewline
\midrule
\endfirsthead
\toprule
Algo & Output1 & Output 2\tabularnewline
\midrule
\endhead
modsk: & 0.00156 \(\pm\) 3.46e-05 & 0.00307 \(\pm\)
5.64e-05\tabularnewline
igp: & 0.0045 \(\pm\) 0.000153 & 0.00787 \(\pm\) 0.000257\tabularnewline
cogp: & 0.00852 \(\pm\) 0.000241 & 0.0149 \(\pm\)
0.000433\tabularnewline
SCGP: & 4.7 \(\pm\) 0.0245 & 3.39 \(\pm\) 0.0192\tabularnewline
npv: & Inf \(\pm\) NaN & Inf \(\pm\) NaN\tabularnewline
mf: & Inf \(\pm\) NaN & Inf \(\pm\) NaN\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lll@{}}
\caption{NLL of the SARCOS dataset (small is better)}\tabularnewline
\toprule
Algo & Output1 & Output 2\tabularnewline
\midrule
\endfirsthead
\toprule
Algo & Output1 & Output 2\tabularnewline
\midrule
\endhead
modsk: & 0.804 \(\pm\) 0.0111 & -0.509 \(\pm\) 0.00813\tabularnewline
igp: & 1.06 \(\pm\) 0.00941 & -0.236 \(\pm\) 0.0124\tabularnewline
cogp: & 2.48 \(\pm\) 0.0577 & 2.4 \(\pm\) 0.097\tabularnewline
SCGP: & 4.63 \(\pm\) 0.0128 & 2.87 \(\pm\) 0.011\tabularnewline
npv: & Inf \(\pm\) NaN & Inf \(\pm\) NaN\tabularnewline
mf: & Inf \(\pm\) NaN & Inf \(\pm\) NaN\tabularnewline
\bottomrule
\end{longtable}

\subsection{DAC14}\label{sec:dac14}

\begin{itemize}
\item
  2000 training
\item
  8000 test
\item
  10 inputs
\item
  15 outputs
\end{itemize}

\begin{longtable}[]{@{}llll@{}}
\caption{SMSE for DAC14 dataset}\tabularnewline
\toprule
Out & MOESK & IGP & COGP\tabularnewline
\midrule
\endfirsthead
\toprule
Out & MOESK & IGP & COGP\tabularnewline
\midrule
\endhead
O1 & 0.00064 \(\pm\) 3.5e-05 & 0.2 \(\pm\) 0.0019 & 0.19 \(\pm\)
0.0033\tabularnewline
O2 & 0.0006 \(\pm\) 3.9e-05 & 0.18 \(\pm\) 0.0061 & 0.15 \(\pm\)
0.0034\tabularnewline
O3 & 0.00054 \(\pm\) 4.2e-05 & 0.084 \(\pm\) 0.0034 & 0.084 \(\pm\)
0.0016\tabularnewline
O4 & 0.00023 \(\pm\) 1.7e-05 & 0.22 \(\pm\) 0.0037 & 0.053 \(\pm\)
0.0014\tabularnewline
O5 & 0.00028 \(\pm\) 2.1e-05 & 0.4 \(\pm\) 0.0053 & 0.087 \(\pm\)
0.0015\tabularnewline
O6 & 0.00022 \(\pm\) 1.4e-05 & 0.24 \(\pm\) 0.0024 & 0.049 \(\pm\)
0.00096\tabularnewline
O7 & 0.0013 \(\pm\) 9.6e-05 & 0.63 \(\pm\) 0.02 & 0.22 \(\pm\)
0.0031\tabularnewline
O8 & 0.0011 \(\pm\) 0.00014 & 0.45 \(\pm\) 0.013 & 0.15 \(\pm\)
0.0017\tabularnewline
O9 & 0.0011 \(\pm\) 0.00013 & 0.12 \(\pm\) 0.0021 & 0.073 \(\pm\)
0.0016\tabularnewline
O10 & 0.0011 \(\pm\) 9.8e-05 & 0.4 \(\pm\) 0.077 & 0.25 \(\pm\)
0.0036\tabularnewline
O11 & 0.001 \(\pm\) 0.00011 & 0.47 \(\pm\) 0.069 & 0.18 \(\pm\)
0.0038\tabularnewline
O12 & 0.00097 \(\pm\) 6.3e-05 & 0.39 \(\pm\) 0.087 & 0.1 \(\pm\)
0.0016\tabularnewline
O13 & 0.00032 \(\pm\) 2.3e-05 & 0.092 \(\pm\) 0.0038 & 0.092 \(\pm\)
0.00096\tabularnewline
O14 & 0.00036 \(\pm\) 2.9e-05 & 0.14 \(\pm\) 0.0052 & 0.11 \(\pm\)
0.0024\tabularnewline
O15 & 0.00027 \(\pm\) 1.4e-05 & 0.059 \(\pm\) 0.00043 & 0.059 \(\pm\)
0.00083\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}llll@{}}
\caption{LL for DAC14 dataset}\tabularnewline
\toprule
Out & MOESK & IGP & COGP\tabularnewline
\midrule
\endfirsthead
\toprule
Out & MOESK & IGP & COGP\tabularnewline
\midrule
\endhead
O1 & -2.3 \(\pm\) 0.033 & 0.43 \(\pm\) 0.0048 & 4.7 \(\pm\)
0.17\tabularnewline
O2 & -2.5 \(\pm\) 0.025 & 0.18 \(\pm\) 0.0079 & 3.3 \(\pm\)
0.25\tabularnewline
O3 & -2.4 \(\pm\) 0.026 & -0.1 \(\pm\) 0.01 & 3.2 \(\pm\)
0.15\tabularnewline
O4 & -3 \(\pm\) 0.038 & -1.1 \(\pm\) 0.018 & 2.1 \(\pm\)
0.14\tabularnewline
O5 & -3.1 \(\pm\) 0.037 & -1.1 \(\pm\) 0.035 & 2.1 \(\pm\)
0.14\tabularnewline
O6 & -3 \(\pm\) 0.03 & -0.95 \(\pm\) 0.011 & 1.8 \(\pm\)
0.21\tabularnewline
O7 & -2.2 \(\pm\) 0.042 & -0.74 \(\pm\) 0.047 & 5.7 \(\pm\)
0.4\tabularnewline
O8 & -2.4 \(\pm\) 0.055 & -1 \(\pm\) 0.041 & 4.8 \(\pm\)
0.36\tabularnewline
O9 & -2.2 \(\pm\) 0.036 & -1 \(\pm\) 0.024 & 2.5 \(\pm\)
0.18\tabularnewline
O10 & -2.1 \(\pm\) 0.052 & 0.32 \(\pm\) 0.045 & 5 \(\pm\)
0.4\tabularnewline
O11 & -2.2 \(\pm\) 0.067 & -0.031 \(\pm\) 0.033 & 4 \(\pm\)
0.13\tabularnewline
O12 & -2.1 \(\pm\) 0.025 & -0.27 \(\pm\) 0.018 & 3.7 \(\pm\)
0.24\tabularnewline
O13 & -2.7 \(\pm\) 0.032 & 0.049 \(\pm\) 0.0063 & 2.7 \(\pm\)
0.16\tabularnewline
O14 & -2.7 \(\pm\) 0.035 & -0.046 \(\pm\) 0.0055 & 2.4 \(\pm\)
0.24\tabularnewline
O15 & -2.7 \(\pm\) 0.023 & -0.2 \(\pm\) 0.0042 & 2.5 \(\pm\)
0.17\tabularnewline
\bottomrule
\end{longtable}
